{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_path=None):\n",
    "    \"\"\"Load raw data from data directory \"data_path\".\n",
    "    Reads text file, converts strings to integer ids\n",
    "    Args:\n",
    "    data_path: string path to the directory\n",
    "    Returns:\n",
    "    tuple (raw_data, vocabulary)\n",
    "    \"\"\"\n",
    "  \n",
    "    data = list(open(data_path, \"r\").read())\n",
    "    counter = collections.Counter(data)\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    chars, _ = list(zip(*count_pairs))\n",
    "    char_to_id = {c:i for i, c in enumerate(chars)}\n",
    "    id_to_char = {i:c for i, c in enumerate(chars)}\n",
    "\n",
    "    data_in_ids = [char_to_id[char] for char in data]\n",
    "    return data, data_in_ids, char_to_id, id_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "raw_data_chars, raw_data_ids, char_to_id, id_to_char = get_data(\"bible_1000.txt\")\n",
    "n_chars = len(char_to_id)\n",
    "print(n_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the length of a sequence of an example's input or response\n",
    "time_steps = 100\n",
    "\n",
    "# create tf.Dataset object\n",
    "dataset = tf.data.Dataset.from_tensor_slices(raw_data_ids)\n",
    "\n",
    "# create examples\n",
    "# NOTE: the \"batch\" defined here is one example (batch of characters) instead of batch of examples\n",
    "examples = dataset.batch(time_steps+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map data to inputs and responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_and_response(example):\n",
    "    example_input = example[:-1]\n",
    "    example_response = example[1:]\n",
    "    return example_input, example_response\n",
    "\n",
    "mapped_dataset = examples.map(input_and_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle and create batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 10000\n",
    "shuffled_dataset = mapped_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension of embedding layer\n",
    "emb_size = 256\n",
    "\n",
    "# number of rnn units\n",
    "rnn_units = 1024\n",
    "\n",
    "# max number of iterations\n",
    "iterations = 500\n",
    "\n",
    "# learning rate\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_chars, emb_size, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(\n",
    "            n_chars,\n",
    "            emb_size, \n",
    "            batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.LSTM(\n",
    "            rnn_units,\n",
    "            return_sequences=True,\n",
    "            stateful=True, \n",
    "            recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(n_chars)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model(\n",
    "    n_chars=n_chars, \n",
    "    emb_size=emb_size,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(responses, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(responses, logits, from_logits=True)\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 102s 3s/step - loss: 3.0650\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (32, None, 256)           15360     \n",
      "_________________________________________________________________\n",
      "unified_lstm (UnifiedLSTM)   (32, None, 1024)          5246976   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (32, None, 60)            61500     \n",
      "=================================================================\n",
      "Total params: 5,323,836\n",
      "Trainable params: 5,323,836\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=1\n",
    "\n",
    "history = model.fit(shuffled_dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate new text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebulid model with batch size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            15360     \n",
      "_________________________________________________________________\n",
      "unified_lstm_1 (UnifiedLSTM) (1, None, 1024)           5246976   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 60)             61500     \n",
      "=================================================================\n",
      "Total params: 5,323,836\n",
      "Trainable params: 5,323,836\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "    n_chars=n_chars, \n",
    "    emb_size=emb_size,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare seed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliel, and Obed, and Jasiel the Mesobaite. \n",
      "Now these are they that came to David to Ziklag, while h\n"
     ]
    }
   ],
   "source": [
    "seed_chars, _, _, _ = get_data(\"bible_seed.txt\")\n",
    "seed_chars = seed_chars[:time_steps]\n",
    "seed_sentence = \"\".join(seed_chars)\n",
    "print(seed_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writer(model, seed_text, written_len, temperature):\n",
    "    \n",
    "    # convert seed text to id list\n",
    "    input_ids = tf.expand_dims([char_to_id[c] for c in seed_text], 0)\n",
    "    \n",
    "    # storage for written text\n",
    "    written_text = []\n",
    "    \n",
    "    model.reset_states()\n",
    "    for k in range(written_len):\n",
    "        pred = tf.squeeze(model(input_ids), 0) / temperature\n",
    "\n",
    "        # predict the last id returned by the model\n",
    "        pred_id = tf.random.categorical(pred, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "        # pass predicted ids as the input of the next prediction\n",
    "        input_ids = tf.expand_dims([pred_id], 0)\n",
    "        \n",
    "        written_text.append(id_to_char[pred_id])\n",
    "        \n",
    "    return (seed_text + \"\".join(written_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BibleLlMdclnpAanr e  e,wbo Cn aandpeauweia , JIwe tR p,D a. nld t eeontotthuAnc s thi twf,thRrh  famalh  '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writer(model, \"Bible\", 100, temperature=1.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
