{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_path=None):\n",
    "    \"\"\"Load raw data from data directory \"data_path\".\n",
    "    Reads text file, converts strings to integer ids\n",
    "    Args:\n",
    "    data_path: string path to the directory\n",
    "    Returns:\n",
    "    tuple (raw_data, vocabulary)\n",
    "    \"\"\"\n",
    "  \n",
    "    data = tf.gfile.GFile(data_path, \"r\").read().replace(\"\\n\", \"<eos>\").split()\n",
    "    \n",
    "    counter = collections.Counter(data)\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    id_to_word = dict(zip(range(len(words)), words))\n",
    "\n",
    "    data_in_ids = [word_to_id[word] for word in data if word in word_to_id]\n",
    "    return data, data_in_ids, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting a one-hot array into a word\n",
    "def vec_to_word(vec, id_to_word):\n",
    "    index = np.argmax(vec, axis=0) # get the index of the most probable word\n",
    "    word = id_to_word[index]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "745\n"
     ]
    }
   ],
   "source": [
    "raw_data_words, raw_data_ids, word_to_id, id_to_word = get_data(\"train.txt\")\n",
    "n_words = len(word_to_id)\n",
    "print(n_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random batch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a random batch from data\n",
    "def get_batch(data, batch_size, time_steps, input_size):\n",
    "    batch = np.zeros([batch_size, time_steps+1, input_size])\n",
    "    for row in range(batch_size):\n",
    "        t0 = np.random.randint(0, len(data)-time_steps) # starting time\n",
    "        batch[row, :, :] = np.eye(input_size)[data[t0:t0+time_steps+1]]\n",
    "    return batch[:, :-1, :], batch[:, 1:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input size\n",
    "batch_size = 5\n",
    "time_steps = 10\n",
    "#epochs = 1 # not considering epoch now\n",
    "\n",
    "# max number of iterations\n",
    "iterations = 200\n",
    "\n",
    "# learning rate\n",
    "n_neurons = 5\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input / Output(target)\n",
    "X = tf.placeholder(tf.float32, [None, time_steps, n_words])\n",
    "Y = tf.placeholder(tf.float32, [None, time_steps, n_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a gru cell\n",
    "cell = tf.nn.rnn_cell.GRUCell(n_words)\n",
    "#cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
    "#    tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu),\n",
    "#    output_size=n_words)\n",
    "\n",
    "# Get gru cell output\n",
    "outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "\n",
    "# Final output layer\n",
    "#dense = tf.layers.dense(inputs=outputs, units=n_words, activation=None)\n",
    "\n",
    "# Class probabilities\n",
    "#probs = tf.nn.softmax(dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss & optimizer\n",
    "#loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=probs))\n",
    "loss = tf.reduce_mean(tf.square(outputs - Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "# Training\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# Initializer\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Save model\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.004818327\n",
      "10 0.2683465\n",
      "20 0.27807683\n",
      "30 0.26325405\n",
      "40 0.24272978\n",
      "50 0.23682836\n",
      "60 0.21883845\n",
      "70 0.21479118\n",
      "80 0.18828242\n",
      "90 0.18007444\n",
      "100 0.18945682\n",
      "110 0.18911353\n",
      "120 0.28000924\n",
      "130 0.16535111\n",
      "140 0.15931684\n",
      "150 0.16522744\n",
      "160 0.15099347\n",
      "170 0.1486667\n",
      "180 0.13406016\n",
      "190 0.10219694\n"
     ]
    }
   ],
   "source": [
    "# Run the model\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        \n",
    "        x_batch, y_batch = get_batch(\n",
    "            data=raw_data_ids,\n",
    "            batch_size=batch_size,\n",
    "            time_steps=time_steps,\n",
    "            input_size=n_words)\n",
    "        \n",
    "        sess.run(train, feed_dict={X: x_batch, Y: y_batch})\n",
    "        \n",
    "        if iteration % 10 == 0:\n",
    "            loss_ = loss.eval(feed_dict={X: x_batch, Y: y_batch})\n",
    "            print(iteration, loss_)\n",
    "    \n",
    "    saver.save(sess, \"./model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate new text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this sentence to start the prediction: \"N years ago the marketing managers were expected researchers.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = ['N', 'years', 'ago', 'the', 'marketing', 'managers', 'were', 'expected', 'researchers', '<eos>']\n",
    "sentence_ids = [word_to_id[word] for word in sentence]\n",
    "seed = np.eye(n_words)[sentence_ids].reshape(1, time_steps, n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10, 745)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_generated_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-285d18dfe4ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mx_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_pred\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(arr, values, axis)\u001b[0m\n\u001b[1;32m   4526\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4527\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4528\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    saver.restore(sess, \"./model\")\n",
    "    \n",
    "    for iteration in range(n_generated_words):\n",
    "        x_pred = seed[:, -time_steps:, :]\n",
    "        y_pred = sess.run(outputs, feed_dict={X: x_pred})\n",
    "        seed = np.append(seed, y_pred[:, -1, :], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
