{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_path=None):\n",
    "    \"\"\"Load raw data from data directory \"data_path\".\n",
    "    Reads text file, converts strings to integer ids\n",
    "    Args:\n",
    "    data_path: string path to the directory\n",
    "    Returns:\n",
    "    tuple (raw_data, vocabulary)\n",
    "    \"\"\"\n",
    "  \n",
    "    data = list(tf.gfile.GFile(data_path, \"r\").read().replace(\"\\n\", \"\").lower())\n",
    "    counter = collections.Counter(data)\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    chars, _ = list(zip(*count_pairs))\n",
    "    char_to_id = dict(zip(chars, range(len(chars))))\n",
    "    id_to_char = dict(zip(range(len(chars)), chars))\n",
    "\n",
    "    data_in_ids = [char_to_id[char] for char in data]\n",
    "    return data, data_in_ids, char_to_id, id_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting a one-hot array into a word\n",
    "def vec_to_char(vec, id_to_char):\n",
    "    index = np.argmax(vec, axis=0) # get the index of the most probable word\n",
    "    char = id_to_char[index]\n",
    "    return char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "raw_data_chars, raw_data_ids, char_to_id, id_to_char = get_data(\"bible_1000.txt\")\n",
    "n_chars = len(char_to_id)\n",
    "print(n_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random batch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a random batch from data\n",
    "def get_batch(data, batch_size, time_steps, input_size):\n",
    "    batch = np.zeros([batch_size, time_steps+1, input_size])\n",
    "    for row in range(batch_size):\n",
    "        t0 = np.random.randint(0, len(data)-time_steps) # starting time\n",
    "        batch[row, :, :] = np.eye(input_size)[data[t0:t0+time_steps+1]]\n",
    "    return batch[:, :-1, :], batch[:, 1:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input size\n",
    "batch_size = 10\n",
    "time_steps = 200\n",
    "#epochs = 1 # not considering epoch now\n",
    "\n",
    "# max number of iterations\n",
    "iterations = 500\n",
    "\n",
    "# learning rate\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input / Output(target)\n",
    "X = tf.placeholder(tf.float32, [None, time_steps, n_chars])\n",
    "Y = tf.placeholder(tf.float32, [None, time_steps, n_chars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a gru cell\n",
    "cell = tf.nn.rnn_cell.LSTMCell(n_chars)\n",
    "\n",
    "# Get gru cell output\n",
    "outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final output layer\n",
    "#dense = tf.layers.dense(inputs=outputs, units=n_chars, activation=None)\n",
    "\n",
    "# Class probabilities\n",
    "#probs = tf.nn.softmax(dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss & optimizer\n",
    "#loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=probs))\n",
    "loss = tf.reduce_mean(tf.square(outputs - Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "# Training\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# Initializer\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Save model\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.031239092\n",
      "10 0.023259386\n",
      "20 0.021676524\n",
      "30 0.020824842\n",
      "40 0.020563284\n",
      "50 0.01998036\n",
      "60 0.019990576\n",
      "70 0.019611323\n",
      "80 0.019201703\n",
      "90 0.019306494\n",
      "100 0.018365458\n",
      "110 0.018932305\n",
      "120 0.01885848\n",
      "130 0.01912328\n",
      "140 0.018751925\n",
      "150 0.018605972\n",
      "160 0.018472342\n",
      "170 0.017465238\n",
      "180 0.018234428\n",
      "190 0.017806942\n",
      "200 0.018157022\n",
      "210 0.018278059\n",
      "220 0.01760551\n",
      "230 0.017699609\n",
      "240 0.017951427\n",
      "250 0.01727437\n",
      "260 0.01785046\n",
      "270 0.017936451\n",
      "280 0.017850341\n",
      "290 0.01729503\n",
      "300 0.017437464\n",
      "310 0.01714337\n",
      "320 0.017585019\n",
      "330 0.01791961\n",
      "340 0.017491262\n",
      "350 0.017071426\n",
      "360 0.017468138\n",
      "370 0.0174489\n",
      "380 0.017486533\n",
      "390 0.017523734\n",
      "400 0.017611077\n",
      "410 0.017182712\n",
      "420 0.016658604\n",
      "430 0.016616503\n",
      "440 0.016483497\n",
      "450 0.016781338\n",
      "460 0.016957931\n",
      "470 0.017228471\n",
      "480 0.017501267\n",
      "490 0.017206803\n"
     ]
    }
   ],
   "source": [
    "# Run the model\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        \n",
    "        x_batch, y_batch = get_batch(\n",
    "            data=raw_data_ids,\n",
    "            batch_size=batch_size,\n",
    "            time_steps=time_steps,\n",
    "            input_size=n_chars)\n",
    "        \n",
    "        sess.run(train, feed_dict={X: x_batch, Y: y_batch})\n",
    "        \n",
    "        if iteration % 10 == 0:\n",
    "            loss_ = loss.eval(feed_dict={X: x_batch, Y: y_batch})\n",
    "            print(iteration, loss_)\n",
    "    \n",
    "    saver.save(sess, \"./model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "these men are peaceable with us; therefore let them dwell in the land, and trade therein; for the land, behold, it is large enough for them; let us take their daughters to us for wives, and let us giv\n"
     ]
    }
   ],
   "source": [
    "seed_chars, _, _, _ = get_data(\"bible_seed.txt\")\n",
    "seed_chars = seed_chars[:time_steps]\n",
    "seed_ids = [char_to_id[char] for char in seed_chars]\n",
    "seed_sentence = \"\".join(seed_chars)\n",
    "print(seed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "pred_iterations = 100\n",
    "new_seed_ids = seed_ids.copy()\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    saver.restore(sess, \"./model/\")\n",
    "    \n",
    "    for iteration in range(pred_iterations):\n",
    "        print(iteration)\n",
    "        x_pred_ids = new_seed_ids[-time_steps:]\n",
    "        x_pred = np.eye(n_chars)[x_pred_ids].reshape(1, time_steps, n_chars)\n",
    "        pred_probs = sess.run(outputs, feed_dict={X: x_pred})\n",
    "        pred_last_index = np.argmax(pred_probs[:, -1:, :], axis=2)[0][0]\n",
    "        pred_last_char = id_to_char[pred_last_index]\n",
    "        new_seed_ids = np.append(new_seed_ids, pred_last_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "these men are peaceable with us; therefore let them dwell in the land, and trade therein; for the land, behold, it is large enough for them; let us take their daughters to us for wives, and let us give the said, and the said, and the said, and the said, and the said, and the said, and the said, and \n"
     ]
    }
   ],
   "source": [
    "final_sentence = \"\".join([id_to_char[id] for id in new_seed_ids])\n",
    "print(final_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
