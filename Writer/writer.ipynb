{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "CHECKPOINT_DIR = './writer_checkpoints'\n",
    "EMBEDDING_SIZE = 256\n",
    "RNN_UNITS = 1024\n",
    "\n",
    "# training\n",
    "DATA_DIR = \"bible.txt\"\n",
    "EPOCHS = 1\n",
    "TIME_STEPS = 200\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "# prediction\n",
    "SEED_TEXT = \"To be honest,\"\n",
    "WRITTEN_LEN = 200\n",
    "TEMPERATURE = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_path=None):\n",
    "    \"\"\"Load raw data from data directory \"data_path\".\n",
    "    Reads text file, converts strings to integer ids\n",
    "    Args:\n",
    "    data_path: string path to the directory\n",
    "    Returns:\n",
    "    tuple (raw_data, vocabulary)\n",
    "    \"\"\"\n",
    "  \n",
    "    data = list(open(data_path, \"r\").read())\n",
    "    counter = collections.Counter(data)\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    chars, _ = list(zip(*count_pairs))\n",
    "    char_to_id = {c:i for i, c in enumerate(chars)}\n",
    "    id_to_char = {i:c for i, c in enumerate(chars)}\n",
    "\n",
    "    data_in_ids = [char_to_id[char] for char in data]\n",
    "    return data, data_in_ids, char_to_id, id_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    }
   ],
   "source": [
    "raw_data_chars, raw_data_ids, char_to_id, id_to_char = get_data(DATA_DIR)\n",
    "n_chars = len(char_to_id)\n",
    "print(n_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tf.Dataset object\n",
    "raw_dataset = tf.data.Dataset.from_tensor_slices(raw_data_ids)\n",
    "\n",
    "# create examples\n",
    "# NOTE: the \"batch\" defined here is one example (batch of characters) instead of batch of examples\n",
    "examples = raw_dataset.batch(TIME_STEPS+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map (to inputs and responses), shuffle, and batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_and_response(example):\n",
    "    example_input = example[:-1]\n",
    "    example_response = example[1:]\n",
    "    return example_input, example_response\n",
    "\n",
    "dataset = examples.map(input_and_response).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_chars, emb_size, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(\n",
    "            n_chars,\n",
    "            emb_size, \n",
    "            batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.LSTM(\n",
    "            rnn_units,\n",
    "            return_sequences=True,\n",
    "            stateful=True, \n",
    "            recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(n_chars)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model(\n",
    "    n_chars=n_chars, \n",
    "    emb_size=EMBEDDING_SIZE,\n",
    "    rnn_units=RNN_UNITS,\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(responses, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(responses, logits, from_logits=True)\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(CHECKPOINT_DIR, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314/314 [==============================] - 1186s 4s/step - loss: 2.0175\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           16128     \n",
      "_________________________________________________________________\n",
      "unified_lstm (UnifiedLSTM)   (64, None, 1024)          5246976   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 63)            64575     \n",
      "=================================================================\n",
      "Total params: 5,327,679\n",
      "Trainable params: 5,327,679\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate new text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebulid model with batch size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "    n_chars=n_chars, \n",
    "    emb_size=EMBEDDING_SIZE,\n",
    "    rnn_units=RNN_UNITS,\n",
    "    batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(CHECKPOINT_DIR))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writer(model, seed, length, temp):\n",
    "    \n",
    "    # convert seed text to id list\n",
    "    input_ids = tf.expand_dims([char_to_id[c] for c in seed], 0)\n",
    "    \n",
    "    # storage for written text\n",
    "    written = []\n",
    "    \n",
    "    model.reset_states()\n",
    "    for k in range(length):\n",
    "        pred = tf.squeeze(model(input_ids), 0) / temp\n",
    "\n",
    "        # predict the last id returned by the model\n",
    "        pred_id = tf.random.categorical(pred, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "        # pass predicted ids as the input of the next prediction\n",
    "        input_ids = tf.expand_dims([pred_id], 0)\n",
    "        \n",
    "        written.append(id_to_char[pred_id])\n",
    "        \n",
    "    return (seed + \"\".join(written))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To be honest, and the king of man and a san raifteres: there this meont beether, and gasto toly and therean. \\nWhen they hath that thather he shall closes us from the accod unto the lanon, Pene of go them. \\nEfter h'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writer(model, SEED_TEXT, WRITTEN_LEN, TEMPERATURE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
